############ Configuration for Ollama ############
ollama:
  models:
    - llama2
############ Configuration for Ollama WebUI ############
ui:
  enabled: true
  replicaCount: 1
  type: "lobe-chat" # Supported values are "open-webui" and "lobe-chat"
  image:
    repository: lobehub/lobe-chat # or use ghcr.io/open-webui/open-webui for "open-webui"
    pullPolicy: IfNotPresent
    tag: "latest"
  service:
    type: ClusterIP
    port: 80
  nodeSelector: {}
  tolerations: {}
  affinity: {}

  ingress:
    enabled: false
    className: ""
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    hosts:
      - host: chart-example.local
        paths:
          - path: /
            pathType: ImplementationSpecific
    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local